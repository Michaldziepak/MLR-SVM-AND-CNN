{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100)\n",
    "Data = pd.read_excel(\"Otw4_mgr.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labeleddata = Data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DCAL</th>\n",
       "      <th>Vsand</th>\n",
       "      <th>Vlime</th>\n",
       "      <th>Vpiryt</th>\n",
       "      <th>Vkerogen</th>\n",
       "      <th>VCL</th>\n",
       "      <th>PHI</th>\n",
       "      <th>CAL</th>\n",
       "      <th>DT</th>\n",
       "      <th>ILD</th>\n",
       "      <th>LL3</th>\n",
       "      <th>GG_Corr</th>\n",
       "      <th>GR</th>\n",
       "      <th>GRS_Corr</th>\n",
       "      <th>NPHICorr</th>\n",
       "      <th>POTA</th>\n",
       "      <th>THOR</th>\n",
       "      <th>URAN</th>\n",
       "      <th>RHOB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.080931</td>\n",
       "      <td>0.686933</td>\n",
       "      <td>0.207863</td>\n",
       "      <td>2.403846e-02</td>\n",
       "      <td>4.241436e-02</td>\n",
       "      <td>0.242686</td>\n",
       "      <td>0.245353</td>\n",
       "      <td>0.020943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035352</td>\n",
       "      <td>0.056986</td>\n",
       "      <td>0.144958</td>\n",
       "      <td>0.185864</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.090516</td>\n",
       "      <td>0.126562</td>\n",
       "      <td>0.471009</td>\n",
       "      <td>0.159040</td>\n",
       "      <td>0.773125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236142</td>\n",
       "      <td>0.929220</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>3.365385e-02</td>\n",
       "      <td>3.752039e-02</td>\n",
       "      <td>0.212101</td>\n",
       "      <td>0.282528</td>\n",
       "      <td>0.263235</td>\n",
       "      <td>0.170292</td>\n",
       "      <td>0.028395</td>\n",
       "      <td>0.042695</td>\n",
       "      <td>0.144906</td>\n",
       "      <td>0.195950</td>\n",
       "      <td>0.270124</td>\n",
       "      <td>0.041231</td>\n",
       "      <td>0.590972</td>\n",
       "      <td>0.620834</td>\n",
       "      <td>0.137118</td>\n",
       "      <td>0.741379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.186253</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>2.884615e-02</td>\n",
       "      <td>1.631321e-03</td>\n",
       "      <td>0.142952</td>\n",
       "      <td>0.260223</td>\n",
       "      <td>0.177297</td>\n",
       "      <td>0.142512</td>\n",
       "      <td>0.027662</td>\n",
       "      <td>0.036797</td>\n",
       "      <td>0.111392</td>\n",
       "      <td>0.144231</td>\n",
       "      <td>0.254960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770486</td>\n",
       "      <td>0.465190</td>\n",
       "      <td>0.089296</td>\n",
       "      <td>0.777504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.343681</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.734421</td>\n",
       "      <td>2.403846e-02</td>\n",
       "      <td>4.323002e-02</td>\n",
       "      <td>0.414894</td>\n",
       "      <td>0.223048</td>\n",
       "      <td>0.417422</td>\n",
       "      <td>0.150968</td>\n",
       "      <td>0.021463</td>\n",
       "      <td>0.033486</td>\n",
       "      <td>0.154396</td>\n",
       "      <td>0.145962</td>\n",
       "      <td>0.188945</td>\n",
       "      <td>0.172718</td>\n",
       "      <td>0.330122</td>\n",
       "      <td>0.319474</td>\n",
       "      <td>0.202546</td>\n",
       "      <td>0.832512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.446785</td>\n",
       "      <td>0.262704</td>\n",
       "      <td>0.719364</td>\n",
       "      <td>2.403846e-02</td>\n",
       "      <td>2.446982e-03</td>\n",
       "      <td>0.049202</td>\n",
       "      <td>0.438662</td>\n",
       "      <td>0.557300</td>\n",
       "      <td>0.134663</td>\n",
       "      <td>0.021196</td>\n",
       "      <td>0.031133</td>\n",
       "      <td>0.046653</td>\n",
       "      <td>0.117085</td>\n",
       "      <td>0.070668</td>\n",
       "      <td>0.087668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384708</td>\n",
       "      <td>0.117317</td>\n",
       "      <td>0.793377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.495565</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.774990</td>\n",
       "      <td>2.019231e-01</td>\n",
       "      <td>8.156607e-04</td>\n",
       "      <td>0.377660</td>\n",
       "      <td>0.126394</td>\n",
       "      <td>0.595846</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>0.025551</td>\n",
       "      <td>0.085438</td>\n",
       "      <td>0.115034</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>0.145778</td>\n",
       "      <td>0.221875</td>\n",
       "      <td>0.746003</td>\n",
       "      <td>0.127475</td>\n",
       "      <td>0.907772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.624169</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.804266</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.301794e-02</td>\n",
       "      <td>0.174202</td>\n",
       "      <td>0.148699</td>\n",
       "      <td>0.520921</td>\n",
       "      <td>0.088166</td>\n",
       "      <td>0.014966</td>\n",
       "      <td>0.026880</td>\n",
       "      <td>0.105990</td>\n",
       "      <td>0.143094</td>\n",
       "      <td>0.072958</td>\n",
       "      <td>0.107580</td>\n",
       "      <td>0.293490</td>\n",
       "      <td>0.235832</td>\n",
       "      <td>0.222844</td>\n",
       "      <td>0.990695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.990799</td>\n",
       "      <td>1.250000e-01</td>\n",
       "      <td>8.156607e-04</td>\n",
       "      <td>0.023936</td>\n",
       "      <td>0.245353</td>\n",
       "      <td>0.611239</td>\n",
       "      <td>0.042272</td>\n",
       "      <td>0.018285</td>\n",
       "      <td>0.032681</td>\n",
       "      <td>0.048528</td>\n",
       "      <td>0.092907</td>\n",
       "      <td>0.088809</td>\n",
       "      <td>0.073734</td>\n",
       "      <td>0.459288</td>\n",
       "      <td>0.151483</td>\n",
       "      <td>0.107906</td>\n",
       "      <td>0.882868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.460089</td>\n",
       "      <td>0.549909</td>\n",
       "      <td>0.310749</td>\n",
       "      <td>2.403846e-02</td>\n",
       "      <td>6.525285e-03</td>\n",
       "      <td>0.318484</td>\n",
       "      <td>0.193309</td>\n",
       "      <td>0.644279</td>\n",
       "      <td>0.118357</td>\n",
       "      <td>0.017089</td>\n",
       "      <td>0.029220</td>\n",
       "      <td>0.109324</td>\n",
       "      <td>0.163193</td>\n",
       "      <td>0.189578</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.133681</td>\n",
       "      <td>0.763287</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.814176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.537694</td>\n",
       "      <td>0.621597</td>\n",
       "      <td>0.119615</td>\n",
       "      <td>6.057692e-01</td>\n",
       "      <td>8.156607e-04</td>\n",
       "      <td>0.444149</td>\n",
       "      <td>0.178439</td>\n",
       "      <td>0.816069</td>\n",
       "      <td>0.237318</td>\n",
       "      <td>0.013633</td>\n",
       "      <td>0.016438</td>\n",
       "      <td>0.108777</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.270649</td>\n",
       "      <td>0.110866</td>\n",
       "      <td>0.554253</td>\n",
       "      <td>0.641796</td>\n",
       "      <td>0.071174</td>\n",
       "      <td>0.933771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.468958</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.851945</td>\n",
       "      <td>8.750000e-01</td>\n",
       "      <td>8.156607e-04</td>\n",
       "      <td>0.158245</td>\n",
       "      <td>0.148699</td>\n",
       "      <td>0.644279</td>\n",
       "      <td>0.091185</td>\n",
       "      <td>0.023875</td>\n",
       "      <td>0.032254</td>\n",
       "      <td>0.053365</td>\n",
       "      <td>0.061386</td>\n",
       "      <td>0.061751</td>\n",
       "      <td>0.055407</td>\n",
       "      <td>0.568663</td>\n",
       "      <td>0.440809</td>\n",
       "      <td>0.162209</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.576497</td>\n",
       "      <td>0.687840</td>\n",
       "      <td>0.137599</td>\n",
       "      <td>3.846154e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.324468</td>\n",
       "      <td>0.319703</td>\n",
       "      <td>0.871179</td>\n",
       "      <td>0.303745</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081891</td>\n",
       "      <td>0.065787</td>\n",
       "      <td>0.297173</td>\n",
       "      <td>0.103002</td>\n",
       "      <td>0.483160</td>\n",
       "      <td>0.722019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.579823</td>\n",
       "      <td>0.295826</td>\n",
       "      <td>0.299038</td>\n",
       "      <td>2.884615e-02</td>\n",
       "      <td>1.631321e-03</td>\n",
       "      <td>0.704787</td>\n",
       "      <td>0.237918</td>\n",
       "      <td>0.585917</td>\n",
       "      <td>0.078504</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.025660</td>\n",
       "      <td>0.152038</td>\n",
       "      <td>0.200697</td>\n",
       "      <td>0.345635</td>\n",
       "      <td>0.237014</td>\n",
       "      <td>0.890972</td>\n",
       "      <td>0.521428</td>\n",
       "      <td>0.084519</td>\n",
       "      <td>0.799945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.331670</td>\n",
       "      <td>0.223756</td>\n",
       "      <td>2.403846e-02</td>\n",
       "      <td>3.588907e-02</td>\n",
       "      <td>0.738032</td>\n",
       "      <td>0.275093</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.232487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>0.174449</td>\n",
       "      <td>0.216841</td>\n",
       "      <td>0.246519</td>\n",
       "      <td>0.254384</td>\n",
       "      <td>0.657899</td>\n",
       "      <td>0.671064</td>\n",
       "      <td>0.219994</td>\n",
       "      <td>0.791735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.730599</td>\n",
       "      <td>0.265880</td>\n",
       "      <td>0.191133</td>\n",
       "      <td>9.519231e-01</td>\n",
       "      <td>2.263814e-16</td>\n",
       "      <td>0.804521</td>\n",
       "      <td>0.178439</td>\n",
       "      <td>0.601353</td>\n",
       "      <td>0.292877</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.020298</td>\n",
       "      <td>0.142704</td>\n",
       "      <td>0.160054</td>\n",
       "      <td>0.338383</td>\n",
       "      <td>0.238404</td>\n",
       "      <td>0.801562</td>\n",
       "      <td>0.627897</td>\n",
       "      <td>0.075986</td>\n",
       "      <td>0.982759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.793792</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.835215</td>\n",
       "      <td>1.923077e-02</td>\n",
       "      <td>8.156607e-04</td>\n",
       "      <td>0.283245</td>\n",
       "      <td>0.260223</td>\n",
       "      <td>0.751117</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.010223</td>\n",
       "      <td>0.033047</td>\n",
       "      <td>0.067484</td>\n",
       "      <td>0.093525</td>\n",
       "      <td>0.225534</td>\n",
       "      <td>0.137189</td>\n",
       "      <td>0.582292</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.035942</td>\n",
       "      <td>0.880952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.776053</td>\n",
       "      <td>0.145644</td>\n",
       "      <td>0.519866</td>\n",
       "      <td>4.134615e-01</td>\n",
       "      <td>8.156607e-04</td>\n",
       "      <td>0.526596</td>\n",
       "      <td>0.208178</td>\n",
       "      <td>0.690543</td>\n",
       "      <td>0.236715</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>0.016118</td>\n",
       "      <td>0.104252</td>\n",
       "      <td>0.157532</td>\n",
       "      <td>0.299539</td>\n",
       "      <td>0.166861</td>\n",
       "      <td>0.550608</td>\n",
       "      <td>0.642815</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.920361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.893570</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.718946</td>\n",
       "      <td>3.750000e-01</td>\n",
       "      <td>8.156607e-04</td>\n",
       "      <td>0.418883</td>\n",
       "      <td>0.260223</td>\n",
       "      <td>0.839223</td>\n",
       "      <td>0.216788</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.018982</td>\n",
       "      <td>0.085073</td>\n",
       "      <td>0.139287</td>\n",
       "      <td>0.165806</td>\n",
       "      <td>0.166273</td>\n",
       "      <td>0.365191</td>\n",
       "      <td>0.374711</td>\n",
       "      <td>0.108820</td>\n",
       "      <td>0.917625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.952739</td>\n",
       "      <td>1.971154e-01</td>\n",
       "      <td>8.156607e-04</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>0.572491</td>\n",
       "      <td>0.962581</td>\n",
       "      <td>0.141909</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>0.229324</td>\n",
       "      <td>0.238542</td>\n",
       "      <td>0.310098</td>\n",
       "      <td>0.006615</td>\n",
       "      <td>0.851122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.190687</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>9.615385e-03</td>\n",
       "      <td>7.846656e-01</td>\n",
       "      <td>0.949468</td>\n",
       "      <td>0.379182</td>\n",
       "      <td>0.066080</td>\n",
       "      <td>0.750605</td>\n",
       "      <td>0.287347</td>\n",
       "      <td>0.251445</td>\n",
       "      <td>0.839678</td>\n",
       "      <td>0.912928</td>\n",
       "      <td>0.789195</td>\n",
       "      <td>0.935323</td>\n",
       "      <td>0.298698</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.732841</td>\n",
       "      <td>0.262452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.221729</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>9.615385e-03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.771277</td>\n",
       "      <td>0.394052</td>\n",
       "      <td>0.088106</td>\n",
       "      <td>0.846620</td>\n",
       "      <td>0.417497</td>\n",
       "      <td>0.409686</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.370226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.797347</td>\n",
       "      <td>0.159278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.086475</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>1.442308e-02</td>\n",
       "      <td>7.936378e-01</td>\n",
       "      <td>0.900266</td>\n",
       "      <td>0.602230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.936595</td>\n",
       "      <td>0.801844</td>\n",
       "      <td>0.713358</td>\n",
       "      <td>0.779920</td>\n",
       "      <td>0.850182</td>\n",
       "      <td>0.628836</td>\n",
       "      <td>0.962124</td>\n",
       "      <td>0.348177</td>\n",
       "      <td>0.716217</td>\n",
       "      <td>0.805453</td>\n",
       "      <td>0.196771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.147450</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>2.884615e-02</td>\n",
       "      <td>8.564437e-01</td>\n",
       "      <td>0.771941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051771</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861681</td>\n",
       "      <td>0.767483</td>\n",
       "      <td>0.827190</td>\n",
       "      <td>0.584046</td>\n",
       "      <td>0.885727</td>\n",
       "      <td>0.186632</td>\n",
       "      <td>0.629589</td>\n",
       "      <td>0.797338</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>2.403846e-02</td>\n",
       "      <td>8.670473e-01</td>\n",
       "      <td>0.829122</td>\n",
       "      <td>0.646840</td>\n",
       "      <td>0.088106</td>\n",
       "      <td>0.991548</td>\n",
       "      <td>0.684272</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827088</td>\n",
       "      <td>0.863186</td>\n",
       "      <td>0.657796</td>\n",
       "      <td>0.796958</td>\n",
       "      <td>0.293924</td>\n",
       "      <td>0.586853</td>\n",
       "      <td>0.811864</td>\n",
       "      <td>0.106459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.291574</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>4.807692e-03</td>\n",
       "      <td>7.887439e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089219</td>\n",
       "      <td>0.266531</td>\n",
       "      <td>0.813407</td>\n",
       "      <td>0.521019</td>\n",
       "      <td>0.428963</td>\n",
       "      <td>0.848634</td>\n",
       "      <td>0.889911</td>\n",
       "      <td>0.749980</td>\n",
       "      <td>0.630732</td>\n",
       "      <td>0.424826</td>\n",
       "      <td>0.859447</td>\n",
       "      <td>0.814981</td>\n",
       "      <td>0.325944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.252772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>5.282233e-16</td>\n",
       "      <td>9.885808e-01</td>\n",
       "      <td>0.853723</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.285262</td>\n",
       "      <td>0.906404</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>0.648876</td>\n",
       "      <td>0.987031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659546</td>\n",
       "      <td>0.740571</td>\n",
       "      <td>0.509896</td>\n",
       "      <td>0.279069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.298030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.260532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.308254e-16</td>\n",
       "      <td>9.828711e-01</td>\n",
       "      <td>0.859043</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.263235</td>\n",
       "      <td>0.904591</td>\n",
       "      <td>0.324234</td>\n",
       "      <td>0.646370</td>\n",
       "      <td>0.981657</td>\n",
       "      <td>0.980272</td>\n",
       "      <td>0.645798</td>\n",
       "      <td>0.751133</td>\n",
       "      <td>0.492969</td>\n",
       "      <td>0.256207</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.302135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.194013</td>\n",
       "      <td>0.820327</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>6.153846e-01</td>\n",
       "      <td>9.461664e-02</td>\n",
       "      <td>0.288564</td>\n",
       "      <td>0.044610</td>\n",
       "      <td>0.095824</td>\n",
       "      <td>0.335749</td>\n",
       "      <td>0.098054</td>\n",
       "      <td>0.108893</td>\n",
       "      <td>0.215710</td>\n",
       "      <td>0.289550</td>\n",
       "      <td>0.334404</td>\n",
       "      <td>0.040787</td>\n",
       "      <td>0.562066</td>\n",
       "      <td>0.531409</td>\n",
       "      <td>0.195319</td>\n",
       "      <td>0.798303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.254989</td>\n",
       "      <td>0.558076</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.035889e-01</td>\n",
       "      <td>0.757979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167411</td>\n",
       "      <td>0.427536</td>\n",
       "      <td>0.042213</td>\n",
       "      <td>0.083863</td>\n",
       "      <td>0.307863</td>\n",
       "      <td>0.400331</td>\n",
       "      <td>0.572592</td>\n",
       "      <td>0.155729</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0.717996</td>\n",
       "      <td>0.163559</td>\n",
       "      <td>0.697865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.208426</td>\n",
       "      <td>0.556261</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>9.615385e-03</td>\n",
       "      <td>6.525285e-02</td>\n",
       "      <td>0.789894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213676</td>\n",
       "      <td>0.408819</td>\n",
       "      <td>0.024077</td>\n",
       "      <td>0.092005</td>\n",
       "      <td>0.261953</td>\n",
       "      <td>0.281441</td>\n",
       "      <td>0.539587</td>\n",
       "      <td>0.165056</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780036</td>\n",
       "      <td>0.118019</td>\n",
       "      <td>0.680624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.200665</td>\n",
       "      <td>0.825318</td>\n",
       "      <td>0.068591</td>\n",
       "      <td>6.105769e-01</td>\n",
       "      <td>8.890701e-02</td>\n",
       "      <td>0.180851</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.272037</td>\n",
       "      <td>0.209542</td>\n",
       "      <td>0.022513</td>\n",
       "      <td>0.049033</td>\n",
       "      <td>0.217411</td>\n",
       "      <td>0.246458</td>\n",
       "      <td>0.391907</td>\n",
       "      <td>0.017606</td>\n",
       "      <td>0.907812</td>\n",
       "      <td>0.389319</td>\n",
       "      <td>0.151120</td>\n",
       "      <td>0.806513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.144124</td>\n",
       "      <td>0.320780</td>\n",
       "      <td>0.402760</td>\n",
       "      <td>1.442308e-02</td>\n",
       "      <td>2.210440e-01</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.490706</td>\n",
       "      <td>0.154186</td>\n",
       "      <td>0.486114</td>\n",
       "      <td>0.070527</td>\n",
       "      <td>0.116903</td>\n",
       "      <td>0.262280</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.237321</td>\n",
       "      <td>0.305519</td>\n",
       "      <td>0.340191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306164</td>\n",
       "      <td>0.529557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DCAL     Vsand     Vlime        Vpiryt      Vkerogen       VCL  \\\n",
       "0   0.080931  0.686933  0.207863  2.403846e-02  4.241436e-02  0.242686   \n",
       "1   0.236142  0.929220  0.001255  3.365385e-02  3.752039e-02  0.212101   \n",
       "2   0.186253  1.000000  0.000836  2.884615e-02  1.631321e-03  0.142952   \n",
       "3   0.343681  0.000454  0.734421  2.403846e-02  4.323002e-02  0.414894   \n",
       "4   0.446785  0.262704  0.719364  2.403846e-02  2.446982e-03  0.049202   \n",
       "5   0.495565  0.000454  0.774990  2.019231e-01  8.156607e-04  0.377660   \n",
       "6   0.624169  0.000454  0.804266  1.000000e+00  5.301794e-02  0.174202   \n",
       "7   0.487805  0.000454  0.990799  1.250000e-01  8.156607e-04  0.023936   \n",
       "8   0.460089  0.549909  0.310749  2.403846e-02  6.525285e-03  0.318484   \n",
       "9   0.537694  0.621597  0.119615  6.057692e-01  8.156607e-04  0.444149   \n",
       "10  0.468958  0.000454  0.851945  8.750000e-01  8.156607e-04  0.158245   \n",
       "11  0.576497  0.687840  0.137599  3.846154e-01  0.000000e+00  0.324468   \n",
       "12  0.579823  0.295826  0.299038  2.884615e-02  1.631321e-03  0.704787   \n",
       "13  0.863636  0.331670  0.223756  2.403846e-02  3.588907e-02  0.738032   \n",
       "14  0.730599  0.265880  0.191133  9.519231e-01  2.263814e-16  0.804521   \n",
       "15  0.793792  0.000454  0.835215  1.923077e-02  8.156607e-04  0.283245   \n",
       "16  0.776053  0.145644  0.519866  4.134615e-01  8.156607e-04  0.526596   \n",
       "17  0.893570  0.000454  0.718946  3.750000e-01  8.156607e-04  0.418883   \n",
       "18  1.000000  0.000454  0.952739  1.971154e-01  8.156607e-04  0.015957   \n",
       "19  0.190687  0.000907  0.001255  9.615385e-03  7.846656e-01  0.949468   \n",
       "20  0.221729  0.000907  0.001255  9.615385e-03  1.000000e+00  0.771277   \n",
       "21  0.086475  0.001361  0.001673  1.442308e-02  7.936378e-01  0.900266   \n",
       "22  0.147450  0.002722  0.002928  2.884615e-02  8.564437e-01  0.771941   \n",
       "23  0.000000  0.002269  0.002091  2.403846e-02  8.670473e-01  0.829122   \n",
       "24  0.291574  0.000454  0.000836  4.807692e-03  7.887439e-01  1.000000   \n",
       "25  0.252772  0.000000  0.000418  5.282233e-16  9.885808e-01  0.853723   \n",
       "26  0.260532  0.000000  0.000000  5.308254e-16  9.828711e-01  0.859043   \n",
       "27  0.194013  0.820327  0.000418  6.153846e-01  9.461664e-02  0.288564   \n",
       "28  0.254989  0.558076  0.000836  0.000000e+00  1.035889e-01  0.757979   \n",
       "29  0.208426  0.556261  0.001255  9.615385e-03  6.525285e-02  0.789894   \n",
       "30  0.200665  0.825318  0.068591  6.105769e-01  8.890701e-02  0.180851   \n",
       "31  0.144124  0.320780  0.402760  1.442308e-02  2.210440e-01  0.281250   \n",
       "\n",
       "         PHI       CAL        DT       ILD       LL3   GG_Corr        GR  \\\n",
       "0   0.245353  0.020943  0.000000  0.035352  0.056986  0.144958  0.185864   \n",
       "1   0.282528  0.263235  0.170292  0.028395  0.042695  0.144906  0.195950   \n",
       "2   0.260223  0.177297  0.142512  0.027662  0.036797  0.111392  0.144231   \n",
       "3   0.223048  0.417422  0.150968  0.021463  0.033486  0.154396  0.145962   \n",
       "4   0.438662  0.557300  0.134663  0.021196  0.031133  0.046653  0.117085   \n",
       "5   0.126394  0.595846  0.013890  0.013230  0.025551  0.085438  0.115034   \n",
       "6   0.148699  0.520921  0.088166  0.014966  0.026880  0.105990  0.143094   \n",
       "7   0.245353  0.611239  0.042272  0.018285  0.032681  0.048528  0.092907   \n",
       "8   0.193309  0.644279  0.118357  0.017089  0.029220  0.109324  0.163193   \n",
       "9   0.178439  0.816069  0.237318  0.013633  0.016438  0.108777  0.134095   \n",
       "10  0.148699  0.644279  0.091185  0.023875  0.032254  0.053365  0.061386   \n",
       "11  0.319703  0.871179  0.303745  0.001204  0.000000  0.081891  0.065787   \n",
       "12  0.237918  0.585917  0.078504  0.010571  0.025660  0.152038  0.200697   \n",
       "13  0.275093  0.865672  0.232487  0.000000  0.022491  0.174449  0.216841   \n",
       "14  0.178439  0.601353  0.292877  0.006645  0.020298  0.142704  0.160054   \n",
       "15  0.260223  0.751117  0.130435  0.010223  0.033047  0.067484  0.093525   \n",
       "16  0.208178  0.690543  0.236715  0.004782  0.016118  0.104252  0.157532   \n",
       "17  0.260223  0.839223  0.216788  0.003168  0.018982  0.085073  0.139287   \n",
       "18  0.572491  0.962581  0.141909  0.004803  0.017722  0.000000  0.000000   \n",
       "19  0.379182  0.066080  0.750605  0.287347  0.251445  0.839678  0.912928   \n",
       "20  0.394052  0.088106  0.846620  0.417497  0.409686  1.000000  0.996020   \n",
       "21  0.602230  0.000000  0.936595  0.801844  0.713358  0.779920  0.850182   \n",
       "22  1.000000  0.051771  1.000000  1.000000  0.861681  0.767483  0.827190   \n",
       "23  0.646840  0.088106  0.991548  0.684272  1.000000  0.827088  0.863186   \n",
       "24  0.089219  0.266531  0.813407  0.521019  0.428963  0.848634  0.889911   \n",
       "25  0.007435  0.285262  0.906404  0.368200  0.648876  0.987031  1.000000   \n",
       "26  0.007435  0.263235  0.904591  0.324234  0.646370  0.981657  0.980272   \n",
       "27  0.044610  0.095824  0.335749  0.098054  0.108893  0.215710  0.289550   \n",
       "28  0.000000  0.167411  0.427536  0.042213  0.083863  0.307863  0.400331   \n",
       "29  0.000000  0.213676  0.408819  0.024077  0.092005  0.261953  0.281441   \n",
       "30  0.029740  0.272037  0.209542  0.022513  0.049033  0.217411  0.246458   \n",
       "31  0.490706  0.154186  0.486114  0.070527  0.116903  0.262280  0.279661   \n",
       "\n",
       "    GRS_Corr  NPHICorr      POTA      THOR      URAN      RHOB  \n",
       "0   0.219653  0.090516  0.126562  0.471009  0.159040  0.773125  \n",
       "1   0.270124  0.041231  0.590972  0.620834  0.137118  0.741379  \n",
       "2   0.254960  0.000000  0.770486  0.465190  0.089296  0.777504  \n",
       "3   0.188945  0.172718  0.330122  0.319474  0.202546  0.832512  \n",
       "4   0.070668  0.087668  0.000000  0.384708  0.117317  0.793377  \n",
       "5   0.160256  0.145778  0.221875  0.746003  0.127475  0.907772  \n",
       "6   0.072958  0.107580  0.293490  0.235832  0.222844  0.990695  \n",
       "7   0.088809  0.073734  0.459288  0.151483  0.107906  0.882868  \n",
       "8   0.189578  0.084900  0.133681  0.763287  0.140962  0.814176  \n",
       "9   0.270649  0.110866  0.554253  0.641796  0.071174  0.933771  \n",
       "10  0.061751  0.055407  0.568663  0.440809  0.162209  1.000000  \n",
       "11  0.297173  0.103002  0.483160  0.722019  0.000000  0.883689  \n",
       "12  0.345635  0.237014  0.890972  0.521428  0.084519  0.799945  \n",
       "13  0.246519  0.254384  0.657899  0.671064  0.219994  0.791735  \n",
       "14  0.338383  0.238404  0.801562  0.627897  0.075986  0.982759  \n",
       "15  0.225534  0.137189  0.582292  0.584211  0.035942  0.880952  \n",
       "16  0.299539  0.166861  0.550608  0.642815  0.037078  0.920361  \n",
       "17  0.165806  0.166273  0.365191  0.374711  0.108820  0.917625  \n",
       "18  0.107017  0.229324  0.238542  0.310098  0.006615  0.851122  \n",
       "19  0.789195  0.935323  0.298698  0.781832  0.732841  0.262452  \n",
       "20  1.000000  1.000000  0.370226  1.000000  0.797347  0.159278  \n",
       "21  0.628836  0.962124  0.348177  0.716217  0.805453  0.196771  \n",
       "22  0.584046  0.885727  0.186632  0.629589  0.797338  0.000000  \n",
       "23  0.657796  0.796958  0.293924  0.586853  0.811864  0.106459  \n",
       "24  0.749980  0.630732  0.424826  0.859447  0.814981  0.325944  \n",
       "25  0.659546  0.740571  0.509896  0.279069  1.000000  0.298030  \n",
       "26  0.645798  0.751133  0.492969  0.256207  0.999263  0.302135  \n",
       "27  0.334404  0.040787  0.562066  0.531409  0.195319  0.798303  \n",
       "28  0.572592  0.155729  0.898438  0.717996  0.163559  0.697865  \n",
       "29  0.539587  0.165056  1.000000  0.780036  0.118019  0.680624  \n",
       "30  0.391907  0.017606  0.907812  0.389319  0.151120  0.806513  \n",
       "31  0.237321  0.305519  0.340191  0.000000  0.306164  0.529557  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(Data.columns.values)\n",
    "Labeleddata_withr8index = Labeleddata.reset_index(drop=True)#NAN DROP\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x = Labeleddata_withr8index.iloc[:,1:20].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "df.columns = features[1:20]\n",
    "\n",
    "non_standarized_data = Labeleddata_withr8index\n",
    "test_row_standarized = df.iloc[5:7]\n",
    "\n",
    "non_standarized_data = non_standarized_data.drop([5,6])\n",
    "TOC = non_standarized_data[\"TOC[%]\"]\n",
    "TOC = TOC.reset_index(drop=True) #31 elements of nonstandarized TOC\n",
    "\n",
    "test_row = Labeleddata_withr8index.iloc[5:7]#test rows 5,6+\n",
    "R8data= df.drop([5,6])# dropping 5 and 6 row in labeled data and creating R8data (31 elements)\n",
    "R8data = R8data.reset_index(drop=True)# reseting index for R8data\n",
    "\n",
    "\n",
    "\n",
    "#TOC = non_standarized_data[\"TOC[%]\"]#toc 31 elements\n",
    "\n",
    "TOC_row5=test_row[\"TOC[%]\"] # TOC 2 elements non standarized\n",
    "#R8data,Toc, test_row_standarized, TOC_row5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 19)\n"
     ]
    }
   ],
   "source": [
    "x, y = R8data, TOC\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 19, 1)\n"
     ]
    }
   ],
   "source": [
    "x = x.values.reshape(x.shape[0],x.shape[1],1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 18, 32)            96        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 37,089\n",
      "Trainable params: 37,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(32, 2, activation=\"relu\", input_shape=(19,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "NAME = \"SIEC-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "10/10 [==============================] - 2s 179ms/step - loss: 0.0560 - accuracy: 0.0000e+00 - val_loss: 3.1334 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.0000e+00 - val_loss: 3.3101 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0454 - accuracy: 0.0000e+00 - val_loss: 3.3780 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0508 - accuracy: 0.0000e+00 - val_loss: 3.1306 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0377 - accuracy: 0.0000e+00 - val_loss: 3.3775 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0389 - accuracy: 0.0000e+00 - val_loss: 3.4244 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0363 - accuracy: 0.0000e+00 - val_loss: 3.2605 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0409 - accuracy: 0.0000e+00 - val_loss: 3.2702 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0387 - accuracy: 0.0000e+00 - val_loss: 3.2126 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0493 - accuracy: 0.0000e+00 - val_loss: 3.4019 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0396 - accuracy: 0.0000e+00 - val_loss: 3.3690 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/150\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 3.2080 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0474 - accuracy: 0.0000e+00 - val_loss: 3.1643 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0589 - accuracy: 0.0000e+00 - val_loss: 3.6077 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0400 - accuracy: 0.0000e+00 - val_loss: 3.2342 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/150\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0397 - accuracy: 0.0000e+00 - val_loss: 3.2795 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0386 - accuracy: 0.0000e+00 - val_loss: 3.2633 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0398 - accuracy: 0.0000e+00 - val_loss: 3.3134 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 3.3968 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0402 - accuracy: 0.0000e+00 - val_loss: 3.3697 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0433 - accuracy: 0.0000e+00 - val_loss: 3.2888 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/150\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0395 - accuracy: 0.0000e+00 - val_loss: 3.3345 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0560 - accuracy: 0.0000e+00 - val_loss: 3.4554 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0418 - accuracy: 0.0000e+00 - val_loss: 3.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/150\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0509 - accuracy: 0.0000e+00 - val_loss: 3.3840 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0398 - accuracy: 0.0000e+00 - val_loss: 3.1003 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0404 - accuracy: 0.0000e+00 - val_loss: 3.2112 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/150\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0373 - accuracy: 0.0000e+00 - val_loss: 3.3330 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/150\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0421 - accuracy: 0.0000e+00 - val_loss: 3.3439 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/150\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 3.1144 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/150\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0431 - accuracy: 0.0000e+00 - val_loss: 3.2678 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/150\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0607 - accuracy: 0.0000e+00 - val_loss: 3.4247 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 0.0000e+00 - val_loss: 3.0808 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0465 - accuracy: 0.0000e+00 - val_loss: 3.2402 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0395 - accuracy: 0.0000e+00 - val_loss: 3.2270 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0402 - accuracy: 0.0000e+00 - val_loss: 3.1056 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0347 - accuracy: 0.0000e+00 - val_loss: 3.3379 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/150\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0460 - accuracy: 0.0000e+00 - val_loss: 3.2062 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/150\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0396 - accuracy: 0.0000e+00 - val_loss: 3.2448 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0437 - accuracy: 0.0000e+00 - val_loss: 3.3560 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0482 - accuracy: 0.0000e+00 - val_loss: 3.1290 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0431 - accuracy: 0.0000e+00 - val_loss: 3.4594 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0382 - accuracy: 0.0000e+00 - val_loss: 3.2871 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0358 - accuracy: 0.0000e+00 - val_loss: 3.1176 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0442 - accuracy: 0.0000e+00 - val_loss: 3.2522 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/150\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0414 - accuracy: 0.0000e+00 - val_loss: 3.2435 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 0.0000e+00 - val_loss: 3.3419 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0456 - accuracy: 0.0000e+00 - val_loss: 3.2574 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0405 - accuracy: 0.0000e+00 - val_loss: 3.0998 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0399 - accuracy: 0.0000e+00 - val_loss: 3.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0365 - accuracy: 0.0000e+00 - val_loss: 3.2143 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0413 - accuracy: 0.0000e+00 - val_loss: 3.2330 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 3.2843 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0585 - accuracy: 0.0000e+00 - val_loss: 3.4245 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0426 - accuracy: 0.0000e+00 - val_loss: 3.1267 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0382 - accuracy: 0.0000e+00 - val_loss: 3.2456 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/150\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0402 - accuracy: 0.0000e+00 - val_loss: 3.4945 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/150\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0387 - accuracy: 0.0000e+00 - val_loss: 3.2758 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - val_loss: 3.2991 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0362 - accuracy: 0.0000e+00 - val_loss: 3.3093 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 3.2756 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0356 - accuracy: 0.0000e+00 - val_loss: 3.3238 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/150\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0422 - accuracy: 0.0000e+00 - val_loss: 3.3186 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.0000e+00 - val_loss: 3.3508 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0427 - accuracy: 0.0000e+00 - val_loss: 3.0467 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0697 - accuracy: 0.0000e+00 - val_loss: 3.5680 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0964 - accuracy: 0.0000e+00 - val_loss: 2.7680 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0529 - accuracy: 0.0000e+00 - val_loss: 3.4890 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0432 - accuracy: 0.0000e+00 - val_loss: 3.3649 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 0.0000e+00 - val_loss: 3.2201 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 3.1777 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0428 - accuracy: 0.0000e+00 - val_loss: 3.3646 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0350 - accuracy: 0.0000e+00 - val_loss: 3.2057 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0468 - accuracy: 0.0000e+00 - val_loss: 3.1410 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.0000e+00 - val_loss: 3.4752 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0371 - accuracy: 0.0000e+00 - val_loss: 3.2914 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0433 - accuracy: 0.0000e+00 - val_loss: 3.3524 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0397 - accuracy: 0.0000e+00 - val_loss: 3.3169 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0570 - accuracy: 0.0000e+00 - val_loss: 3.1964 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0434 - accuracy: 0.0000e+00 - val_loss: 3.0555 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0367 - accuracy: 0.0000e+00 - val_loss: 3.2218 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.0000e+00 - val_loss: 3.3164 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0419 - accuracy: 0.0000e+00 - val_loss: 3.3099 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.0000e+00 - val_loss: 3.0910 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0567 - accuracy: 0.0000e+00 - val_loss: 3.3854 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3867e-05 - accuracy: 0.0000e+ - 0s 8ms/step - loss: 0.0315 - accuracy: 0.0000e+00 - val_loss: 3.1016 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0426 - accuracy: 0.0000e+00 - val_loss: 3.3031 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0445 - accuracy: 0.0000e+00 - val_loss: 3.2835 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0393 - accuracy: 0.0000e+00 - val_loss: 3.1393 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.0000e+00 - val_loss: 3.2757 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0434 - accuracy: 0.0000e+00 - val_loss: 3.2882 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0379 - accuracy: 0.0000e+00 - val_loss: 3.2546 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 3.2906 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0561 - accuracy: 0.0000e+00 - val_loss: 3.2952 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0390 - accuracy: 0.0000e+00 - val_loss: 3.1695 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 0.0000e+00 - val_loss: 3.4026 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0304 - accuracy: 0.0000e+00 - val_loss: 3.0866 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0482 - accuracy: 0.0000e+00 - val_loss: 3.4375 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/150\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0420 - accuracy: 0.0000e+00 - val_loss: 3.3143 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0571 - accuracy: 0.0000e+00 - val_loss: 3.1061 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0744 - accuracy: 0.0000e+00 - val_loss: 3.6548 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0497 - accuracy: 0.0000e+00 - val_loss: 2.9655 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0377 - accuracy: 0.0000e+00 - val_loss: 3.2286 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0331 - accuracy: 0.0000e+00 - val_loss: 3.4796 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0521 - accuracy: 0.0000e+00 - val_loss: 3.2236 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0411 - accuracy: 0.0000e+00 - val_loss: 3.0824 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0635 - accuracy: 0.0000e+00 - val_loss: 3.2828 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 3.0738 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0407 - accuracy: 0.0000e+00 - val_loss: 3.2218 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0362 - accuracy: 0.0000e+00 - val_loss: 3.4554 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0449 - accuracy: 0.0000e+00 - val_loss: 3.4433 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0495 - accuracy: 0.0000e+00 - val_loss: 3.0434 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0317 - accuracy: 0.0000e+00 - val_loss: 3.2326 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0313 - accuracy: 0.0000e+00 - val_loss: 3.3792 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0398 - accuracy: 0.0000e+00 - val_loss: 3.2248 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0347 - accuracy: 0.0000e+00 - val_loss: 3.1898 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0351 - accuracy: 0.0000e+00 - val_loss: 3.2898 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0319 - accuracy: 0.0000e+00 - val_loss: 3.2140 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.0000e+00 - val_loss: 3.1675 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0385 - accuracy: 0.0000e+00 - val_loss: 3.3052 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0327 - accuracy: 0.0000e+00 - val_loss: 3.2670 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/150\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0474 - accuracy: 0.0000e+00 - val_loss: 3.2449 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0326 - accuracy: 0.0000e+00 - val_loss: 3.3813 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - val_loss: 3.1048 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0453 - accuracy: 0.0000e+00 - val_loss: 3.1030 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0627 - accuracy: 0.0000e+00 - val_loss: 3.4766 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 0.0000e+00 - val_loss: 2.9555 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0566 - accuracy: 0.0000e+00 - val_loss: 3.3404 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0460 - accuracy: 0.0000e+00 - val_loss: 3.1031 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0464 - accuracy: 0.0000e+00 - val_loss: 3.3162 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/150\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0341 - accuracy: 0.0000e+00 - val_loss: 3.1056 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0391 - accuracy: 0.0000e+00 - val_loss: 3.1166 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0525 - accuracy: 0.0000e+00 - val_loss: 3.2690 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.0000e+00 - val_loss: 2.9588 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/150\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0370 - accuracy: 0.0000e+00 - val_loss: 3.2996 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0359 - accuracy: 0.0000e+00 - val_loss: 3.1315 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0327 - accuracy: 0.0000e+00 - val_loss: 3.1559 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0400 - accuracy: 0.0000e+00 - val_loss: 3.2105 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0398 - accuracy: 0.0000e+00 - val_loss: 3.3025 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/150\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0319 - accuracy: 0.0000e+00 - val_loss: 2.9545 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0481 - accuracy: 0.0000e+00 - val_loss: 3.1696 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0354 - accuracy: 0.0000e+00 - val_loss: 3.1095 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0387 - accuracy: 0.0000e+00 - val_loss: 3.1635 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0337 - accuracy: 0.0000e+00 - val_loss: 3.0753 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0330 - accuracy: 0.0000e+00 - val_loss: 3.2420 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/150\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0463 - accuracy: 0.0000e+00 - val_loss: 2.9730 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0503 - accuracy: 0.0000e+00 - val_loss: 3.1241 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0466 - accuracy: 0.0000e+00 - val_loss: 3.2441 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/150\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0311 - accuracy: 0.0000e+00 - val_loss: 3.0068 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/150\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0423 - accuracy: 0.0000e+00 - val_loss: 3.0469 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22abda090a0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain, ytrain,batch_size = 2,epochs=150,validation_split=0.2,callbacks =[tensorboard] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1520 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3253825 ],\n",
       "       [0.2801219 ],\n",
       "       [5.6602073 ],\n",
       "       [0.5620396 ],\n",
       "       [0.23084085],\n",
       "       [0.5808277 ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = model.predict(xtest)\n",
    "ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_ax = range(len(ypred))\n",
    "plt.scatter(x_ax, ytest, s=10, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, ypred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12812413]\n",
      " [-0.15018119]]\n"
     ]
    }
   ],
   "source": [
    "test_rownet = np.asarray(test_row_standarized) #testrow to model data without H, Hlab and TOC\n",
    "test_rownet = test_rownet.reshape(2,19,1)\n",
    "testpred = model.predict(test_rownet)\n",
    "print(testpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
